---
title: |
  | \vspace{7cm} STAT 6021: Project Two
subtitle: "Medical Insurance Costs"
author: "Niraja Bhidar(nd4dg), Derek Banks(dmb3ey), Jay Hombal (mh4ey), Ronak Rijhwani (rr7wq)"
header-includes:
    - \usepackage{setspace}\doublespacing
output:
  pdf_document:
    fig_height: 5
    fig_width: 6
  html_document:
    df_print: paged
  html_notebook:
    fig_height: 5
    fig_width: 6
editor_options: 
  chunk_output_type: console
---

\pagenumbering{gobble}
\centering
\raggedright
\clearpage
\pagenumbering{arabic}

```{r setup, include=FALSE, message=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE, comment='')
```

```{r,include=FALSE}
##Bring in all needed packages
library(leaps)
library(MASS)
library(tidyverse)
library(grid)
library(gridExtra)
library(psych)
library(car)
library(ROCR)
```

```{r, include=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plot list (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plot  list
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout 
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

## 1 Executive Summary :

Insurance companies calculate premiums by using models based on certain demographics. We wanted to ascertain what variables influenced claims and how influential these predictors were. Through Kaggle, we were able to obtain a data set that contained the claims of people based on age, body mass index (BMI), sex, region, smoking, and children. 

At first, we individually compared each variable against charges and saw that our data appeared to be right skewed. This implied that a transformation of the data would be necessary in order to normalize our data. Before we did that, we wanted to discern which variables were significant to keep in our model.  

The linear regression output for our initial model, which contained all the variables, showed the northwest region was linearly related some other variable, therefore suggesting it should be dropped from the model. However, no matter how we manipulated the predictors, the data seemed to have non-constant variance which would give no value to any statistical analysis we did on these models. Due to the complexity of data manipulation, we used a logarithmic binomial model.  

In this approach, we split the groups into charges that were above 20,000 USD dollars and charges that were below 20,000. The equation we found to be the best fit for our data was:

$$ \ln \frac{\pi_i}{1-\pi_i} = -7.86104 + 0.04087 age + 0.10134bmi + 4.75564smokeryes $$ 

This shows that the odds of having charges over $20,000 gets multiplied by a factor of 116 if one is a smoker (while holding age and BMI constant), gets multiplied by a factor 1.13 for each BMI increase (while holding age and smoking constant), and gets multiplied by a factor of 1.04 for every addition year (while holding smoking and BMI constant).     

## 2 Exploratory Data Analysis :

The data was contained in the file datasets_13720_18513_insurance.csv included with this project.

* The variables were as follows:
  + **Predictors**
    - **x1**:	**age**: age of primary beneficiary.
    - **x2**: **sex**: insurance contractor gender, female, male.
    - **x3**:	**bmi**: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9.
    - **x4**: **children**: Number of children covered by health insurance / Number of dependents.
    - **x5**: **smoker**: Smoking
    - **x6**:	**region**: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.
  + **Response Variable**
    - **Y**: **charges**: Individual medical costs billed by health insurance.


* **The main objectives for this project were: **  
  1. Explore relationship between response variable **charges** & the six other predictor variables (x1â€“x6).  
  2. Analyze the correlation and directionality of the dataset.  
  3. Create a model that is the best fit model to predict the insurance **charges** based the demographic predictor variables and evaluate the validity and usefulness of this model.  
  
Additionally, we planned to utilize model selection tools to give us a deeper understanding of how different potential models compare. We want to recommend a best fit model and end our section by exploring the pros and cons of our models under consideration.

Exploratory data analysis started with investigating the dataset.

```{r echo=TRUE}
data <- read.csv("datasets_13720_18513_insurance.csv", header = TRUE, sep =",", 
                 stringsAsFactors = TRUE)
head(data)
```


There were six predictors and a response variable **charges**.  The dataset had 1338 rows, and the data appeared to need little cleaning and did not contain missing values.

```{r include=FALSE}
unique(sapply(data, is.na))
```

```{r echo=FALSE}
str(data)
```

Inspecting the data types of variables, we observed that the predictor variables sex, smoker, and region were categorical variables and were automatically converted as a factor by R when loading the dataset because because of the option **stringsAsFactors = TRUE** used while reading the csv file

```{r echo=FALSE}
summary(data)
summary(data$charges)
```

From the summary we made the following observations :

  - The observations seemed to be evenly distributed across region.
  - The age varied between low of 18 and a max of 64.
  - The observations were almost evenly distributed by sex.
  - The dataset had almost 4:1 non-smoker to smoker ratio or only 20.5% people smoke.
  - The bmi varied between  a min of 15.96 and max of 53.13.
  
The response variable mean was greater than median, this was an indication that data is right-skewed. This could be confirmed by the histogram of **charges** shown below. 


```{r echo=FALSE, message=FALSE, fig.height=3, fig.width=5, warning=FALSE}

hg1 <- ggplot(data=data, aes(data$charges)) +
  geom_histogram(colour = "darkblue", fill = "lightblue" ) +
  ggtitle("Histogram for Charges") +
  theme_classic() +  
  xlab("Charges") +
  theme(plot.title = element_text(hjust = 0.5))

multiplot(hg1, cols=2)

```

In the box plot shown below for medical **charges** by **sex** the median value of the medical **charges** for both male and female appeared to be almost the same. The third quartile for male seemed to be greater than female, so the data may be skewed towards the men.

The box plot of medical **charges** by number of **children**, we made an interesting observation that the medical **charges** for people with 5 children were lower than people with one to four children and people with no children had the lowest medical charges. 

```{r echo=FALSE, fig.height=5.5, fig.width=10, message=FALSE, warning=FALSE}

g1 <- ggplot(data = data, aes(sex,charges)) + 
  geom_boxplot(fill = c(2:3)) +
  theme_classic() +  
  xlab("sex") +
  ggtitle("Boxplot of Medical Charges by Gender") + 
  theme(plot.title = element_text(hjust = 0.5))

g2 <- ggplot(data = data,aes(as.factor(children),charges)) + 
  geom_boxplot(fill = c(2:7)) +
  theme_classic() +  
  xlab("children") +
  ggtitle("Boxplot of Medical Charges by Number of Children") +
  theme(plot.title = element_text(hjust = 0.5))

g3 <- ggplot(data = data,aes(region,charges)) + 
  geom_boxplot(fill = c(2:5)) +
  theme_classic() +
  xlab("US Region") +
  ggtitle("Boxplot of Medical Charges per Region") +
  theme(plot.title = element_text(hjust = 0.5))

g4 <- ggplot(data = data, aes(smoker,charges)) + 
  geom_boxplot(fill = c(2:3)) +
  xlab("Smoking Staus") +
  theme_classic() + ggtitle("Boxplot of Medical Charges by Smoking Status") +
  theme(plot.title = element_text(hjust = 0.5))

multiplot(g1, g2, cols=2)

```
  
In the box plot of medical **charges** per **region** the median value of the medical **charges** across all four US regions was almost the same. The people in the southeast seemed to have higher medical expenses then the people in the other areas. 

However, exploring the box plot of medical **charges** by **smoking** status, we could see that the medical **charges** for those who smoke were much higher than those who do not smoke.   

```{r echo=FALSE, fig.height=5.5, fig.width=10, message=FALSE, warning=FALSE}
multiplot(g3, g4, cols=2)
```

The Correlation matrix:  
```{r echo=FALSE, message=FALSE, warning=FALSE}
#create a scatter plot matrix of all our quantitative variables
cor(data[c("charges", "age", "bmi", "children")])
```

We observed that **age** and **charges** were moderately correlated, therefore as age increased, the medical charges also increased moderately.  There was also a moderate correlation between **age** and **bmi**, and **children** and **charges**

```{r echo=FALSE}
pairs.panels(data [ c("charges", "age", "bmi", "children") ])
``` 


#### Computational Exploration

A model with all predictors was considered as an initial starting point. Additional candidate models were calculated by applying model automatic predictor search procedures.

The R^2^~adj~ value and the BIC metrics were used to identify likely models since these both approaches penalized for adding more terms.

```{r message=FALSE, warning=FALSE, include=FALSE}
#take a look at all the first-order subset regression models
allreg <-regsubsets(data$charges~., data=data, nbest=10)

##create a "data frame" that stores the predictors in the various models considered as well as their various criteria

best <- as.data.frame(summary(allreg)$outmat)
best$p <- as.numeric(substr(rownames(best),1,1))+1
best$r2 <- summary(allreg)$rsq
best$adjr2 <- summary(allreg)$adjr2
best$mse <- (summary(allreg)$rss)/(dim(data)[1]-best$p)
best$cp <- summary(allreg)$cp
best$bic <- summary(allreg)$bic

```

The following models were the two automatic search procedure recommended models:
 
    + The model with lowest BIC (-1817.233) was: $$ charges = \beta_0 + \beta_1 age + \beta_2 bmi + \beta_3 children + \beta_4 smokeyes $$
 
  + The model with highest adjusted R^2^ was: $$ charges = \beta_0 + \beta_1 age + \beta_2 bmi + \beta_3 children + \beta_4 smokeyes + \beta_5 regionsoutheast + \beta_6 region5southwest $$

```{r include=FALSE}
best %>% top_n(-1, bic)
```
 

```{r include=FALSE}
best %>% top_n(1, adjr2)
```

We also considered the models with the highest R^2^, lowest Cp, and lowest MSE values. The best Cp and best MSE were both on the the same model as the best adjusted R^2^

```{r include=FALSE}
best %>% top_n(-1, cp)
```

```{r include=FALSE}
best %>% top_n(-1, mse)
```

The model with the best R^2^ value has all predictors as adjusted R^2^ in additon to regionnorthwest

```{r include=FALSE}
best %>% top_n(1, r2)
```

##### Summary of Exploratory Data Analysis:

The following observations were made from the exploratory data analysis phase:

1. The smokers had more medical expenses than non-smokers
2. None of the correlations from the correlation matrix appeared to be strong
3. The quantitative predictors **age**, **bmi**, and **children** were moderately correlated with response variable
4. From computational analysis, we observed that categorical variable sex and region may be considred as significant predictors.
5. There may be a possibility of the data set being skewed, particularly charges 

## 3. Initial Model Considered:

Based on the results from the model search procedures, the intial model considered was:
$$ charges = \beta_0 + \beta_1 age + \beta_2 bmi + \beta_3 children + \beta_4 smokeyes + \beta_5 region + \beta_6 sex $$

```{r echo=TRUE}
initalmodel <- lm(charges ~ age + bmi + children + smoker + region +sex, data=data)
summary(initalmodel)
```

Next the linear regression assumptions were validated:

```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))

residualPlot(initalmodel, type = "rstandard")
abline(h=0, col="orange")

boxcox(initalmodel, main = "Box-Cox")

##ACF plot of residuals
acf(initalmodel$residuals, main="ACF of Residuals")

##Normal probability or QQ plot of residuals
qqnorm(initalmodel$residuals, col='blue')
qqline(initalmodel$residuals, col="orange")
```

In the plots displayed above, we observed that the variance was not constant as seen in the box-cox plot and that the residual plot did not have constant variance. 

In our hypothesis, we stated that, age, people who smoke and people with high bmi (bmi>30) may be at high risk and so their medical costs may be higher. Based on that hypothesis and considering that our initial model suffered from non-linearity and non-constant variance issues. We will transformed both response variable and predictors.

The following transformations were applied:
  1. Transformed **charges** (y) to fix non-constant variance
  2. Transformed age by adding a non-linear term
  3. Created a indicator variable for bmi (obesity indicator)
  4. Specified an interaction between smokers and bmi indicator predictor

```{r echo=FALSE}
data$age2 <- data$age^2

#The **bmi** above 30 is an indicator of obesity, so we create a new indicator variable bmi30 is 1 if it is at least 30 or 0 if less.

if (is.factor(data$bmi) != TRUE)
  {
    data$bmi30 <- ifelse(data$bmi >= 30, 1, 0)
    data$bmi30 <-factor(data$bmi30)
  }
is.factor(data$bmi30)
transformed.model <- lm(charges^0.15 ~ age + age2 + children + bmi + sex + bmi30 * smoker + region , data=data)
summary(transformed.model)
```

Multiple R^2^ and Adjusted R^2^ measured how well our model explained the response variable. The transformed model had higher Multiple R^2^ = 0.8063 and Adjusted R^2^ = 0.8047 compared to initial model Multiple R^2^ = 0.7509 and Adjusted R^2^ = 0.7494 

We also observed from the model summary, age2 the second order variable is insignificant based on t value and high p-value greater than 0.05. The interaction term bmi301:somkeryes is significant.

The next step was to verify the linear regression model assumptions:

```{r echo=FALSE, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
par(mfrow=c(2,2))

#plot(initalmodel.y$fitted.values,initalmodel.y$residuals, main="Residual Plot", col='blue')
#abline(h=0, col="orange")
residualPlot(transformed.model, type = "rstandard")

boxcox.lambda <- boxcox(transformed.model, main = "Box-Cox", col='blue',lambda=seq(-1,4, by=0.1))

##ACF plot of residuals
acf(transformed.model$residuals, main="ACF of Residuals", col='blue')

##Normal probability or QQ plot of residuals
qqnorm(transformed.model$residuals, col='blue')
qqline(transformed.model$residuals, col="orange")
```


While box cox plot now showed that non-constant variance issue was addressed, the residual plot still appeaed to have non-constant variance. It was not clear that the transform solved the non-constant and non-linearity issue, we further explored which predictors could be removed by creating partial regression plots.


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=5,  fig.width=6}
par(mfrow=c(2,2))
#parital regression plot for bmi
result.y.bmi <- lm(charges^0.15 ~ age + children + smoker  + region, data=data)
res.y.bmi <- result.y.bmi$residuals
result.bmi <- lm(bmi~ age + children + smoker  + region, data=data)
res.bmi <- result.bmi$residuals
plot(res.bmi,res.y.bmi, main="parital regression plot of bmi")
abline(h=0)
abline(lm(res.y.bmi~res.bmi),col="red")

#parital regression plot for children
result.y.children <- lm(charges^0.15 ~ age  + smoker  + region, data=data)
res.y.children <- result.y.children$residuals
result.children <- lm(children~ age  + smoker  + region, data=data)
res.children <- result.children$residuals
plot(res.children,res.y.children, main="parital regression plot of children")
abline(h=0)
abline(lm(res.y.children~res.children),col="red")

#parital regression plot for age
result.y.age <- lm(charges ~ children  + smoker  + region + bmi, data=data)
res.y.age <- result.y.age$residuals
result.age <- lm(age ~ children  + smoker  + region +bmi, data=data)
res.age <- result.age$residuals
plot(res.age,res.y.age, main="parital regression plot of age")
abline(h=0)
abline(lm(res.y.age~res.age), col="red")

```

From the partial regression plot, we observed a leaner pattern for all three quantiative variables, this means the linear terms for the predictors **bmi**, **age** and **children** seemed appropriate


```{r echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
str(data)
full.sqrt.age2 <- lm(charges ^ .15 ~ log(age) + children + log(bmi) + smoker + region, data=data)
summary(full.sqrt.age2)

par(mfrow=c(2,2))

plot(full.sqrt.age2$fitted.values,full.sqrt.age2$residuals, main="Residual Plot", col='blue')
abline(h=0, col="orange")

boxcox.lambda <- boxcox(full.sqrt.age2, main = "Box-Cox", col='blue',lambda=seq(-1,4, by=0.1))

##ACF plot of residuals
acf(full.sqrt.age2$residuals, main="ACF of Residuals", col='blue')

##Normal probability or QQ plot of residuals
qqnorm(full.sqrt.age2$residuals, col='blue')
qqline(full.sqrt.age2$residuals, col="orange")

```

```{r echo=FALSE, fig.height=5}
##residuals
result <- full.sqrt.age2 
res<-result$residuals 

##studentized residuals
student.res<-rstandard(result) 

##externally studentized residuals
ext.student.res<-rstudent(result) 

n = nrow(swiss)

# 3 predictors and intercept
p = 5 + 1

##critical value using Bonferroni procedure
qt(1-0.05/(2*n), n-p-1)

par(mfrow=c(2,2))
plot(result$fitted.values,res,main="Residuals")
plot(result$fitted.values,student.res,main="Studentized Residuals")
plot(result$fitted.values,ext.student.res,main="Externally  Studentized Residuals")
plot(ext.student.res,main="Externally Studentized Residuals", ylim=c(-4,4))
abline(h=qt(1-0.05/(2*n), n-p-1), col="red")
abline(h=-qt(1-0.05/(2*n), n-p-1), col="red")

#sort(ext.student.res)
ext.student.res[abs(ext.student.res)>qt(1-0.05/(2*n), n-p-1)]
```

Even after applying transformations, the model fit is still did not seem to satisfy linear regression assumptions. We still observed the presence of non-linearity and non-constant variance potentially due to outliers in the data. This model may be adequate to explore the relationship between the predictors and the response variable. However, the predicted values may be unrealistic.

We observed that the scatter plot of charges against age had three distinct relationships, where the medical charges increased with age at a very slight increasing rate in three segments. Since this relationship was odd, we wished to explore if age was the reason for skew in the data.

```{r fig.height=4, fig.width=4}
plot(data$age, data$charges)
```

Age was removed from the model and the and the response variable was transformed. When validating the assumptions through a box-cox plot, non-constant variation was still present thus not making it a good model for consideration

```{r}
without.age <- lm(charges^.35 ~ + children + smoker + sex + region + bmi, data=data)
summary(without.age)

```

However, since age independently had the highest correlation with charges, the adjusted R-squared value fell to 0.5813 in the model without a period. Therefore, we do not believe it made sense to use this model as a predictor, especially given the significant trade-off in predictability.

```{r}
par(mfrow=c(2,2))
# without.age = without age and with y transformed to achieve lambda = 1 maximized
plot(without.age$fitted.values,without.age$residuals, main="Residual Plot", col='blue')
abline(h=0, col="orange")
boxcox.lambda <- boxcox(without.age, main = "Box-Cox", col='blue',lambda=seq(-1,4, by=0.1))
##ACF plot of residuals
acf(without.age$residuals, main="ACF of Residuals", col='blue')
##Normal probability or QQ plot of residuals
qqnorm(without.age$residuals, col='blue')
qqline(without.age$residuals, col="orange")
```


#### Initial Model Summary:

We increased the adjusted R^2^ for our model by transforming the initial model we considered, however the non-constant variance and non-linearity issues in the data set were not fully addressed. We acknowledged that our initial model could be used to explore the relationship between response and predictor variables. However, predictions may not be accurate. 

## 4 Alternate Models Considered:

Based on the analysis of the linear regression model, the goal was modified to find a model that predicted the charges to be above or below a certian threshold value. For example above or below $20,000.  A logistic regression was then considered instead of linear regression.   

The response variable was convereted to a categorical variable and the data was split the data into a training and a testing dataset.


```{r include=FALSE}
if (is.factor(data$bmi) != TRUE)
{
    lrdata <- mutate(data, lrcharges = if_else(charges <= 20000, 0, 1))
    lrdata$lrcharges <-factor(lrdata$lrcharges)
}
is.factor(lrdata$logit_charges)


str(lrdata)
set.seed(199)
n_train <- floor(0.5 * nrow(lrdata))
train_indices <- sample(1:nrow(lrdata), n_train, replace=F)
lrdata_train <- lrdata[train_indices, ]
lrdata_test <- lrdata[-train_indices, ]

```


```{r}
lrmodel1<-glm(lrcharges ~ age + bmi + smoker + region + sex + children, family="binomial",
              data = lrdata_train)
summary(lrmodel1)
```

The higher the difference between null deviance and residual deviance, the better the model's predictability would be. Our data supported the claim that our logistic regression model was useful in estimating the log odds of whether medical **charges** are greater or less than $20000

The model summary showed, based Z-value (Wald test) age, bmi, and smoker are significant predictors with p-value of less than 0.05. Furthermore, the region sex and children predictors seem insignificant, hence removing the model.

Hypothesis-testing:  H~0~: coefficients for all predictors is = 0 and   
                     H~1~: at least one coefficient is not zero
                    
```{r}
1-pchisq(lrmodel1$null.deviance - lrmodel1$deviance,8)
```

Based on the small p-value we rejected the null hypothesis that at least one of these coefficients was not zero.

```{r}
lrmodel2 <-glm(lrcharges ~ age + bmi  + smoker, family="binomial" , data = lrdata_train)
summary(lrmodel2)
```

An obersvation from the Wald test was that the region, sex, and children predictors were insignificant, so we will conducted the delta G^2^ test to see if these predictors can be removed from the model.


```{r}
#test if additional predictors have coefficients equal to 0
1-pchisq(lrmodel2$deviance - lrmodel1$deviance,5)
```

The p-value was 0.0941 greater than 0.05, so we could not reject the null. We then chose a simpler model with just the three predictors **age**, **bmi** and **smoker**.

#### Logistic Regression model validation  

We then tested how well this logistic regression model performed in predicting an outcome that medical **charges** were greater than or less than $20000 given the values of other predictors, using the probability of the observations in the test data of being in each class, we will choose a threshold of 0.5 for the confusion matrix.    

```{r include=FALSE}
predsr2<- predict(lrmodel2,newdata=lrdata_test, type='response')
ratesr2 <- prediction(predsr2, lrdata_test$lrcharges)
roc_resultr2 <- performance(ratesr2, measure = 'tpr', x.measure = 'fpr')
plot(roc_resultr2)
lines(x=c(0,1), y=c(0,1), col="red")
aucr2 <- performance(ratesr2, measure = 'auc')
aucr2@y.values
lrdata_test$lrchareges
table(lrdata_test$lrcharges, predsr2>0.5)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
FP = 33
TP = 110
TN = 495
FN = 31
overallerror = (FP+FN/(FP+FN+TP+TN))
overallerror
FPR = (FP/ (TN+FP))
FPR
FNR = (FN/(FN+TP))
FNR
sensitivity = 1- FNR
sensitivity
sepecifity= 1- FPR
sepecifity
```


False Positive Rate: When it was actually no, how often would it predict yes  = 0.0625

False Negative Rate: When it was actually yes, how often would it predict yes = 0.2198582  

Sensitivity  out of all the positive classes, how much was predicted correctly = 0.7801418

Specificity determined the proportion of actual negatives that were correctly identified = 0.9375


The AUC value for our model was 0.8999704. The AUC value was higher than 0.5, which meant the model did better than random guessing the classifying observations.

## 5. Conclusion:


1. Even after applying transformations, the model fit is still did not satisfy linear regression assumptions.

2. There were still observed non-linearity, and non-constant variance issues thate not addressed in the linear model.

3. The regression assumption issues could be due to skewed data or outliers in the dataset.

4. The conclusion was that our initial transformed model is useful for exploring the relationship between predictor and response variables. However, the predicted values would be unreliable.

5. The data is skewed when it comes to age & smokers, producing more balanced dataset may improve the predictability of our initial MLR model.

6. The alternate logistic regression model would be a better predictor of the likelihood of charges above 20,000 USD when other variables were held constant.


#### The recommendation would be to use with the logistic regression model for better predictability.

$$\pi = ln(P(charges>20000)==1) $$

$$ \ln \frac{\pi_i}{1-\pi_i} = -7.86104 + 0.04087 age + 0.10134bmi + 4.75564smokeryes $$





